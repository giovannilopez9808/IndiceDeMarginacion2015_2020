\subsubsection{Análisis de componentes principales \label{sec:pca}}

El análisis de componentes principales (PCA) es un método estadístico que permite simplificar la complejidad de espacios muestrales con muchas dimensiones a la vez que conserva su información. La idea del algoritmo es asociar a cada elemento de la base de datos con un vector de menor dimensión tal que se minimiza la ecuación \ref{eq:pca_idea}.

\begin{equation}
    \sum_i \sum_j (d(x_i,x_j)^2-d(x^*_i-x_j^*)^2)^2
    \label{eq:pca_idea}
\end{equation}

El problema de minimizar la ecuación \ref*{eq:pca_idea} se puede transformar a la ecuación \ref{eq:pca_problem}.

\begin{equation}
    min\;\; ||-\frac{1}{2}\mathbb{C}(\mathbb{D}^2-\mathbb{D}^{*2})\mathbb{C}||_F^2
    \label{eq:pca_problem}
\end{equation}

Donde $\mathbb{C}$ es la matriz para centrar. Definiendo a la matriz kernel como $\mathbb{K}=\mathbb{X}\mathbb{X}^t$ y $\mathbb{D}$ como la matriz asociada a las distancias, la ecuación \ref{eq:pca_problem} puede escribirse como en la ecuación \ref{eq:pca_cost_function}.

\begin{equation}
    min\;\; ||\mathbb{K}_c-\mathbb{K}^*|| \qquad \mathbb{K}_c = \mathbb{C}\mathbb{K}\mathbb{C}
    \label{eq:pca_cost_function}
\end{equation}

La solución de la ecuación \ref{eq:pca_cost_function} es $\mathbb{K}=\sum\limits_i^p \lambda_i v_i v_i^t $, donde $v_i$ y $\lambda_i$ son los eigenvectores y eigenvalores de $\mathbb{K}$ respectivamente.

\paragraph{Kernel lineal}

El kernel lineal se encuentra definido en la ecuación \ref{eq:kernel_lineal}.

\begin{equation}
    K(x,x') = x \cdot x'
    \label{eq:kernel_lineal}
\end{equation}

\paragraph{Kernel coseno}

El kernel coseno se encuentra definido en la ecuación \ref{eq:kernel_coseno}.

\begin{equation}
    K(x,x') = \frac{x\cdot x'}{||x|| ||x'||}
    \label{eq:kernel_coseno}
\end{equation}

Donde $r$ es un parámetro y $d$ es el grado del polinomio.

\paragraph{Kernel gaussiano}

El kernel gaussiano se encuentra definido en la ecuación \ref{eq:kernel_gaussiano}.

\begin{equation}
    K(x,x') = exp(- \gamma ||x - x'||^2)
    \label{eq:kernel_gaussiano}
\end{equation}

Donde $\gamma$ es un parámetro que controla el comportamiento del kernel. Cuando es muy pequeño el modelo se aproxima al kernel lineal.

\paragraph{Kernel sigmoide}

El kernel sigmoide se encuentra definido en la ecuación \ref{eq:kernel_sigmoide}.

\begin{equation}
    K(x,x') = tanh(\gamma (x \cdot x') +r)
    \label{eq:kernel_sigmoide}
\end{equation}


\paragraph{Parámetros}

En la tabla \ref{table:pca_parameters} se encuentran los parámetros que se usaron para cada kernel antes especificado.

\begin{table}[H]
    \centering
    \begin{tabular}{cccc} \hline
        Kernel    & Grado & $\gamma$      & $r$ \\ \hline
        Lineal    & -     & -             & -   \\
        Coseno    & -     & -             & -   \\
        Gaussiano & -     & $\frac{1}{n}$ & -   \\
        Sigmoide  & -     & $\frac{1}{n}$ & 1   \\ \hline
    \end{tabular}
    \caption{Parámetros usados para cada kernel. El simbolo $-$ indica que no es necesario el parámetro en el kernel.}
    \label{table:pca_parameters}
\end{table}